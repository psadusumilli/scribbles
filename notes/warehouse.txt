'Data WareHouse':
#---------------------------------------------------------------------------------------------------------------

'Overview':
A data warehouse is a system that extracts, cleans, conforms, and delivers
source data into a dimensional data store and then supports and implements
querying and analysis for the purpose of decision making

	
	SALES-DB	|--|						|REPORT		
	INVENTORY-DB|--|--ETL-->DATAWAREHOUSE-->|OLAP (star schema and data cubes)
	LOGISTICS-DB|--|						|DATA MINING 

'ETL':
******
to get data out of the source and load it into the data warehouse

2 parallel threads
	Requirements & Reality=>Architecture=>System Implementation=>Test & Release
	Extract=>Clean=>Conform=>Deliver

#--------------------------------------------------------------------------------------------------------
'CHAPTER 1'
***********

'Requirements'
**************
1|Business needs=>
	information required to make wise decisions,
	lots of interviews with stakeholders 
	a moving target that must be revisited often
2|Compliance=>
	Archived copies of data sources and subsequent stagings of data
	Proof of the complete transaction flow that changed any data
	Fully documented algorithms for allocations and adjustments
	Proof of security of the data copies over time, both on-line and off-line
3|Data Profiling=>
	systematic examination of the quality, scope, and context of a data source to allow an ETL system to be built.
	Corrective steps for bad data source:
		Elimination of some input fields completely
		Flagging of missing data and generation of special surrogate keys
		Best-guess automatic replacement of corrupted values
		Human intervention at the record level
		Development of a full-blown normalized representation of the data
4|Security=>
	best to use LDAP roles that GRANT privileges on table for end-users
	ETL team will have read-write privileges but in a secure firewalled environment.
5|Data Integration=>
	dimensions and fact must be agreed upon for the entire enterprise across systems
6|Data Latency=>
	time taken for final data to reach end end-users
	if urgent, the architecture of the ETL system must convert from batch oriented to streaming oriented
7|Archiving and staging=>
	Staging must be done at every stage of Extract, Clean, Conform, Deliver
	Conservartive approach is to also archive (store in permanent media for retrieval) at every stage.
8|End User delivery interfaces=>
	must not leave a complicated normalised schema for the end-users
9|Talent available
	Decide to build in-house solution or buy a tool 

'Architecture'
**************
ETL tools and Hand Coding have advantages/disadvantages but authors prefer tools.
Batch (delay in sending data to end-users) vs streaming (care must be exercised)
Horizontal (paralle jobs with no merging dimensions) vs Vertical task dependency.
Scheduler vs manual jobs
exception handling should be uniform
Restart and recovery must be kept in mind from design start
Security at every level.

'Front room'=>end-users area, querying,reporting, analysis.
'Back room'=>data sources, etl engines, dimensional tables.

Back room offloads the ETL teams of the following duties whieh are required in the Front room.
	Providing detailed security at a row, column, or applications level
	Building query performance-enhancing indexes and aggregations
	Providing continuous up-time under service-level agreements
	Guaranteeing that all data sets are consistent with each other

'4 stages of ETL':
	'extract'=>
		copy the data from source as-is without conversions, 
		can be re-read multiple times in case of process failures downstream
		can be discarded after end-users data is ready.
	'cleaning'=>
		validation and business rules applied, 
		duplicate data removed
		this cleaned data can be fed back into sources to improve their quality
	'conforming'=>
		merging labels and data types
	'delivery'=>
		final star schema, OLAP cubes

'ODS: Operational Data Store'
	transit data store fed from transactional sources that can act either as
		DW datasource 
		a ad-hoc querying system for end-users when DW data is in processing mode.
	mostly assimilated inside modern DWs as they become more streaming.

'Datamart':
	belongs to Front Room,
	a subset of dimensional tables belonging to a subject area
	supports drilling data down to the lowest level.

'EDW: enterprise data warehouse'
	non-dimensional, normalised, custom-schema, data warehouse for a enterprise.

#--------------------------------------------------------------------------------------------------------
'CHAPTER 2'
***********
'Staging':
its upto architect to decide whether to persist or do everything in memory to avoid IO cost.
provides means of backup and restore.
helps in auditing
belongs to the ETL team only 
volumetric tracking system must be enabled for each staging area, gives table growth estimates, sizes, etl jobs to DBAs.
	'flat files'=>
		data is stored as pure text files in tabular form
		faster since no database overhead, 
		use os utilities like 'grep, sort, tr' to act on data 
		no index
	'xml'=>
		best for data transfer than storage
	"rdbms"=>
		database features like schema, index, sql
		keep it simple, no relations to any existing data sources.

'dimensional schema'
	'fact table' aka measurement table(sale) with keys of 'dimensional tables'(time, person, product)
	each 'dimensional table' has a surrogate key(PK mostly integer) that is related to fact table via foreign key
	fact tables can be atomic or aggregate based on need
	data cubes are formed using the fact and dimensions.
	'surrogate key mapping tables' contains the mapping of surrogate key to the original key from data sources.
	




















	


























