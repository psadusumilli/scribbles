QUICKIE:
********

tar -xzf kafka_2.10-0.8.2.0.tgz
cd kafka_2.10-0.8.2.0
bin/zookeeper-server-start.sh config/zookeeper.properties
bin/kafka-server-start.sh config/server.properties

#single-node:
bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test
bin/kafka-topics.sh --list --zookeeper localhost:2181
bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test 
bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning

#multi-node 
----------- 
config/server-1.properties:
    broker.id=1
    port=9093
    log.dir=/tmp/kafka-logs-1
 
config/server-2.properties:
    broker.id=2
    port=9094
    log.dir=/tmp/kafka-logs-2

bin/kafka-server-start.sh config/server-1.properties &
bin/kafka-server-start.sh config/server-2.properties &
bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topic
bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic
	topic:my-replicated-topic	PartitionCount:1	ReplicationFactor:3	Configs:
		Topic: my-replicated-topic	Partition: 0	Leader: 1	Replicas: 1,2,0	Isr: 1,2,0
	"leader" is the node responsible for all reads and writes for the given partition. Each node will be the leader for a randomly selected portion of the partitions.
	"replicas" is the list of nodes that replicate the log for this partition regardless of whether they are the leader or even if they are currently alive.
	"isr" is the set of "in-sync" replicas. This is the subset of the replicas list that is currently alive and caught-up to the leader.

produce messages to one broker 
bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-replicated-topic

#test failover
--------------
ps -ef | grep server-1.properties
kill -9 <pid>

bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic
Topic:my-replicated-topic	PartitionCount:1	ReplicationFactor:3	Configs:
	Topic: my-replicated-topic	Partition: 0	Leader: 2	Replicas: 1,2,0	Isr: 2,0


---------------------------------------------------------------------------------------------------------------------

PRINCIPLES:
**********
'topic' is a logical unit for creation/consumption of messages
topic is made up of 'partitions', an append-only,immutable commit logs
'producers' select a partition to dump a message based on their distribution choice.
messages in partitions are retained based on retention policies
1 complete partition per server
all partitions are replicated across servers
1 partition in a server is leader handling all read/writes, others passively replicated for failover

'consumers' read from partitions using 'offset', just a counter on where they are in the log
consumers can read multiple times from same parition by resetting the offset
consumers are grouped together into groups

only one consumer is guranteed to receive a message in a group, no duplicate receipt
'point': if all consumers belong to same group, they are load balanced within group for every message
'broadcast': if all consumers belong to different groups , they are distributed across groups for every message

1 partition/1 consumer always -> strong ordering

Messages sent by a producer to a particular topic partition will be appended in the order they are sent. 
That is, if a message M1 is sent by the same producer as a message M2, and M1 is sent first, then M1 will have a lower offset than M2 and appear earlier in the log.
A consumer instance sees messages in the order they are stored in the log.
For a topic with replication factor N, we will tolerate up to N-1 server failures without losing any messages committed to the log.
there cannot be more consumer instances than partitions.

GIST
****
topic -> 
	'Parallelism' - N partitions [1 partition/server, 1 partition/consumer (strong ordering)]
	'Replication' - Each partition in N is replicated across N servers with one as leader for R/W, other just Read

---------------------------------------------------------------------------------------------------------------------

ARCHITECTURE
**************
Persistence:
-------------
The key fact about disk performance is that the throughput of hard drives has been diverging from the latency of a disk seek for the last decade. 
As a result the performance of linear writes on a JBOD configuration with six 7200rpm SATA RAID-5 array is about 600MB/sec but the performance of random writes is only about 100k/sec—a difference of over 6000X. 
These linear reads and writes are the most predictable of all usage patterns, and are heavily optimized by the operating system. 
A modern operating system provides read-ahead and write-behind techniques that prefetch data in large block multiples and group smaller logical writes into large physical writes. 

All disk reads and writes will go through the disk cache in RAM.
In-process memory cache like JVM heap suffer from big/many objects and GC, so pagecache looks better.
Disk cache will stay warm even if the service is restarted, whereas the in-process cache will need to be rebuilt in memory 

Constant time O(1)
------------------
Traditional messagsing systems use B-tree O(LogN) which coupled with disk speed (say 10ms a pop) is pretty slow.
A simple write-at-head, read-at-tail without blocking each other gives O(N)
help utlilize cheap slow-seek drives

Efficiency
----------
Once poor disk access patterns have been eliminated, there are two common causes of inefficiency in this type of system: 
	too many small I/O operations, and excessive byte copying.

'solution: batch copy - too many io ops'
To avoid this, our protocol is built around a "message set" abstraction that naturally groups messages together. 
This allows network requests to group messages together and amortize the overhead of the network roundtrip rather than sending a single message at a time. 
The server in turn appends chunks of messages to its log in one go, and the consumer fetches large linear chunks at a time.	

'solution: custom binary format - '
client and server share common binary format
The message log maintained by the broker is itself just a directory of files, each populated by a sequence of message sets that have been written to disk in the same format used by the producer and consumer.
Maintaining this common format allows optimization of the most important operation: network transfer of persistent log chunks. 
Modern unix operating systems offer a highly optimized code path for transferring data out of pagecache to a socket; in Linux this is done with the 'sendfile' system call.

To understand the impact of sendfile, it is important to understand the common data path for transfer of data from file to socket:
	The operating system reads data from the disk into pagecache in kernel space
	The application reads the data from kernel space into a user-space buffer
	The application writes the data back into kernel space into a socket buffer
	The operating system copies the data from the socket buffer to the NIC buffer where it is sent over the network

This is clearly inefficient, there are four copies and two system calls. 
Using sendfile, this re-copying is avoided by allowing the OS to send the data from pagecache to the network directly. 
So in this optimized path, only the final copy to the NIC buffer is needed.

We expect a common use case to be multiple consumers on a topic.
Using the zero-copy optimization above, data is copied into pagecache exactly once and reused on each consumption instead of being stored in memory and copied out to kernel space every time it is read. 
This combination of 'pagecache and sendfile' means that on a Kafka cluster where the consumers are mostly caught up you will see no read activity on the disks whatsoever as they will be serving data entirely from cache.


Compression:
-----------
To save network io cost, efficient compression requires compressing multiple messages together rather than compressing each message individually.
Kafka supports this by allowing recursive message sets. A batch of messages can be clumped together compressed and sent to the server in this form. 
This batch of messages will be written in compressed form and will remain compressed in the log and will only be decompressed by the consumer.
gzip and snappy

producer
--------
Producers always talk to Brokers who abtract everything like which partition is the leader, which server is alive.
But producer cna do the partition mechanism and dump in any partition they want.
'Batching' is one of the big drivers of efficiency, and to enable batching the Kafka producer will attempt to accumulate data in memory and to send out larger batches in a single request. 
The batching can be configured to accumulate no more than a fixed number of messages and to wait no longer than some fixed latency bound (say 64k or 10 ms).

consumer:
---------
'Pull based' consuming giving control to consumers for batch pulls
advantage of a pull-based system is that it lends itself to aggressive batching of data sent to the consumer.
The deficiency of a naive pull-based system is that if the broker has no data the consumer may end up polling in a tight loop, effectively busy-waiting for data to arrive. 
To avoid this we have parameters in our pull request that allow the consumer request to block in a "long poll" waiting until data arrives 
(and optionally waiting until a given number of bytes is available to ensure large transfer sizes).

'No message acknowledgement' mechanism which leads to inefficiency and lost messages tracking. 
Just track the int offset number provided by consumer

A consumer can deliberately rewind back to an old 'offset' and re-consume data. 
This violates the common contract of a queue, but turns out to be an essential feature for many consumers
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Message Semantics
****************
In general, in any messaging system, the following Semantics are possible.
'At most once'—Messages may be lost but are never redelivered.
'At least once'—Messages are never lost but may be redelivered.
'Exactly once'—this is what people actually want, each message is delivered once and only once.

Kafka guarantees 'at-least-once' delivery by default.
But allows the user to implement 'at most once' delivery by disabling retries on the producer and committing its offset prior to processing a batch of messages. 
'Exactly-once delivery' requires co-operation with the destination storage system but Kafka provides the offset which makes implementing this straight-forward

Replication
***********
'X' Topic can have 'Y' Partitions across 'Z' Brokers
For X, Y > Z
Only one Leader handling R/W in Y partitions
Leader chooses the ordering of values provided to it.
Followers are just like any consumer feeding from leader

For Kafka node liveness or being in 'sync' has two conditions
	A node must be able to maintain its session with ZooKeeper (via ZooKeeper heartbeat mechanism)
	If it is a slave it must replicate the writes happening on the leader and not fall "too far" behind
 
Quorum
------
A message is considered "committed" when all in sync  'ISR' replicas for that partition have applied it to their log.	
 
In general, for a quorum :
 Let say we have 2f+1 replicas. 
 If f+1 replicas must receive a message prior to a commit being declared by the leader, 
 and if we elect a new leader by electing the follower with the most complete log from at least f+1 replicas, then, with no more than f failures, 
 the leader is guaranteed to have all committed messages. 
 This is because among any f+1 replicas, there must be at least one replica that contains all committed messages.

 To tolerate 1 failure requires 3 copies of the data, 
 and to tolerate 2 failures requires 5 copies of the data. 
 In our experience having only enough redundancy to tolerate a single failure is not enough for a practical system, 
 but doing every write five times, with 5x the disk space requirements and 1/5th the throughput, is not very practical for large volume data problems. 

 'ISR'
 Kafka takes a slightly different approach to choosing its quorum set. 
 Instead of majority vote, Kafka dynamically maintains a set of in-sync replicas (ISR) that are caught-up to the leader. 
 Only members of this set are eligible for election as leader. 
 A write to a Kafka partition is not considered committed until all in-sync replicas have received the write. 
 This ISR set is persisted to ZooKeeper whenever it changes. 
 Because of this, any replica in the ISR is eligible to be elected leader.
 This is an important factor for Kafka usage model where there are many partitions and ensuring leadership balance is important. 
 With this ISR model and 'f+1' replicas, a Kafka topic can tolerate 'f' failures without losing committed messages.
 
 'isr failures'
 A failed ISR must fully re-sync again even if it lost unflushed data in its crash.
 In case of all ISR failures
	 1 Wait for a replica in the ISR to come back to life and choose this replica as the leader (hopefully it still has all its data).
	 2 Choose the first replica (not necessarily in the ISR) that comes back to life as the leader.
 
 Log Compaction
 ***************

 